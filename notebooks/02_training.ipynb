{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E-Commerce Customer Churn Prediction\n",
    "## Model Training & Evaluation\n",
    "\n",
    "**Author:** Muhammad Abdullah  \n",
    "**Project:** ML Fundamentals - Customer Churn Prediction\n",
    "\n",
    "---\n",
    "\n",
    "### Objectives:\n",
    "1. Preprocess and engineer features\n",
    "2. Train multiple ML models with MLflow tracking\n",
    "3. Perform hyperparameter tuning\n",
    "4. Evaluate and compare models\n",
    "5. Generate SHAP explanations\n",
    "6. Save best model for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import project modules\n",
    "from config import get_config, MODELS_DIR, PROCESSED_DATA_DIR, FIGURES_DIR\n",
    "from src.data import DataLoader, DataPreprocessor\n",
    "from src.features import FeatureEngineer\n",
    "from src.models import ModelTrainer, ModelEvaluator, ModelExplainer\n",
    "\n",
    "print('All modules imported successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = get_config()\n",
    "print('Configuration loaded:')\n",
    "print(f\"- Target column: {config['data']['target_column']}\")\n",
    "print(f\"- Test size: {config['data']['test_size']}\")\n",
    "print(f\"- Random state: {config['data']['random_state']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "loader = DataLoader(config)\n",
    "\n",
    "# Try to load processed data, fallback to raw\n",
    "try:\n",
    "    df = loader.load_processed_data('eda_data.parquet')\n",
    "    print('Loaded processed data from EDA')\n",
    "except FileNotFoundError:\n",
    "    print('Loading raw data...')\n",
    "    df = loader.load_raw_data()\n",
    "\n",
    "print(f'Data shape: {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data\n",
    "validation = loader.validate_data(df)\n",
    "print('Data Validation Results:')\n",
    "print(f\"- Total rows: {validation['total_rows']}\")\n",
    "print(f\"- Total columns: {validation['total_columns']}\")\n",
    "print(f\"- Duplicates: {validation['duplicates']}\")\n",
    "print(f\"- Target distribution: {validation.get('target_distribution', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "fe = FeatureEngineer(config)\n",
    "\n",
    "# Create all features\n",
    "df_features = fe.create_all_features(df)\n",
    "\n",
    "print(f'Original features: {len(df.columns)}')\n",
    "print(f'After feature engineering: {len(df_features.columns)}')\n",
    "print(f'\\nNew features created: {fe.get_created_features()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = DataPreprocessor(config)\n",
    "\n",
    "# Clean data\n",
    "df_clean = preprocessor.clean_data(df_features)\n",
    "\n",
    "# Handle missing values\n",
    "df_imputed = preprocessor.handle_missing_values(df_clean)\n",
    "\n",
    "# Handle outliers (clip method)\n",
    "df_processed = preprocessor.handle_outliers(df_imputed, method='iqr', action='clip')\n",
    "\n",
    "print(f'Processed data shape: {df_processed.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "target_col = config['data']['target_column']\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = loader.get_train_test_split(\n",
    "    df_processed,\n",
    "    target_col=target_col,\n",
    "    stratify=True\n",
    ")\n",
    "\n",
    "print(f'Training set: {X_train.shape}')\n",
    "print(f'Validation set: {X_val.shape}')\n",
    "print(f'Test set: {X_test.shape}')\n",
    "print(f'\\nChurn rate - Train: {y_train.mean():.2%}, Val: {y_val.mean():.2%}, Test: {y_test.mean():.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform features\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_val_transformed = preprocessor.transform(X_val)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "feature_names = preprocessor.get_feature_names()\n",
    "print(f'Transformed features: {len(feature_names)}')\n",
    "print(f'Feature names: {feature_names[:10]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessor and feature names\n",
    "joblib.dump(preprocessor.preprocessor, MODELS_DIR / 'preprocessor.joblib')\n",
    "joblib.dump(feature_names, MODELS_DIR / 'feature_names.joblib')\n",
    "print('Preprocessor and feature names saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model trainer\n",
    "trainer = ModelTrainer(config)\n",
    "\n",
    "# Train all models\n",
    "print('Training all models with MLflow tracking...')\n",
    "print('=' * 50)\n",
    "\n",
    "models = trainer.train_all_models(\n",
    "    X_train_transformed, y_train,\n",
    "    X_val=X_val_transformed, y_val=y_val\n",
    ")\n",
    "\n",
    "print(f'\\nTrained {len(models)} models: {list(models.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble model\n",
    "ensemble = trainer.create_ensemble(\n",
    "    X_train_transformed, y_train,\n",
    "    models=['random_forest', 'xgboost', 'lightgbm'],\n",
    "    method='voting'\n",
    ")\n",
    "print('Ensemble model created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune XGBoost\n",
    "print('Tuning XGBoost hyperparameters...')\n",
    "best_xgb, best_params = trainer.hyperparameter_tuning(\n",
    "    X_train_transformed, y_train,\n",
    "    model_name='xgboost',\n",
    "    n_trials=30  # Reduce for faster execution\n",
    ")\n",
    "\n",
    "print(f'\\nBest XGBoost parameters: {best_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator(config)\n",
    "\n",
    "# Evaluate all models on test set\n",
    "print('Model Performance on Test Set:')\n",
    "print('=' * 60)\n",
    "\n",
    "comparison_df = evaluator.evaluate_all_models(\n",
    "    trainer.get_all_trained_models(),\n",
    "    X_test_transformed,\n",
    "    y_test\n",
    ")\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparison\n",
    "evaluator.plot_model_comparison(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "evaluator.plot_roc_curves(\n",
    "    trainer.get_all_trained_models(),\n",
    "    X_test_transformed, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curves\n",
    "evaluator.plot_precision_recall_curves(\n",
    "    trainer.get_all_trained_models(),\n",
    "    X_test_transformed, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_name, best_model, best_score = trainer.get_best_model(\n",
    "    X_val_transformed, y_val,\n",
    "    metric='f1'\n",
    ")\n",
    "\n",
    "print(f'Best Model: {best_name}')\n",
    "print(f'Best F1 Score: {best_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for best model\n",
    "evaluator.plot_confusion_matrix(\n",
    "    best_model,\n",
    "    X_test_transformed, y_test,\n",
    "    model_name=best_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print('Classification Report:')\n",
    "print('=' * 50)\n",
    "report = evaluator.get_classification_report(\n",
    "    best_model,\n",
    "    X_test_transformed, y_test\n",
    ")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal threshold\n",
    "optimal_threshold, best_f1 = evaluator.find_optimal_threshold(\n",
    "    best_model,\n",
    "    X_val_transformed, y_val,\n",
    "    metric='f1'\n",
    ")\n",
    "print(f'Optimal threshold: {optimal_threshold:.2f} (F1: {best_f1:.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Explainability (SHAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize explainer\n",
    "explainer = ModelExplainer(config)\n",
    "\n",
    "# Setup SHAP explainer\n",
    "explainer.setup_shap_explainer(\n",
    "    best_model,\n",
    "    X_train_transformed[:100],  # Use subset for background\n",
    "    model_type='tree'\n",
    ")\n",
    "\n",
    "print('SHAP explainer initialized!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate SHAP values\n",
    "shap_values = explainer.calculate_shap_values(\n",
    "    X_test_transformed[:500],  # Use subset for speed\n",
    "    feature_names=feature_names\n",
    ")\n",
    "\n",
    "print(f'SHAP values calculated: {shap_values.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot\n",
    "explainer.plot_shap_summary(X_test_transformed[:500], max_display=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Bar Plot (Feature Importance)\n",
    "explainer.plot_shap_bar(X_test_transformed[:500], max_display=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance ranking\n",
    "importance_df = explainer.get_feature_importance_shap()\n",
    "print('Top 15 Most Important Features (SHAP):')\n",
    "importance_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single prediction explanation\n",
    "sample_idx = 0\n",
    "sample = X_test_transformed[sample_idx:sample_idx+1]\n",
    "\n",
    "# Get risk factors\n",
    "risk_factors = explainer.get_top_risk_factors(sample, top_n=5)\n",
    "print('Top Risk Factors for Sample Customer:')\n",
    "risk_factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "model_path = trainer.save_model(best_model, 'best_model')\n",
    "print(f'Best model saved to: {model_path}')\n",
    "\n",
    "# Also save with specific name\n",
    "trainer.save_model(best_model, best_name)\n",
    "\n",
    "# Save all trained models\n",
    "for name, model in trainer.get_all_trained_models().items():\n",
    "    trainer.save_model(model, name)\n",
    "    \n",
    "print('All models saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model metadata\n",
    "metadata = {\n",
    "    'best_model': best_name,\n",
    "    'metrics': comparison_df.loc[best_name].to_dict(),\n",
    "    'optimal_threshold': optimal_threshold,\n",
    "    'feature_names': feature_names,\n",
    "    'n_features': len(feature_names),\n",
    "    'training_samples': len(X_train),\n",
    "}\n",
    "\n",
    "joblib.dump(metadata, MODELS_DIR / 'model_metadata.joblib')\n",
    "print('Model metadata saved!')\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('TRAINING SUMMARY')\n",
    "print('='*60)\n",
    "\n",
    "print(f'''\n",
    "DATASET:\n",
    "  - Total samples: {len(df_processed)}\n",
    "  - Features (after engineering): {len(feature_names)}\n",
    "  - Train/Val/Test split: {len(X_train)}/{len(X_val)}/{len(X_test)}\n",
    "\n",
    "MODELS TRAINED:\n",
    "  - {list(trainer.get_all_trained_models().keys())}\n",
    "\n",
    "BEST MODEL: {best_name}\n",
    "  - F1 Score: {best_score:.4f}\n",
    "  - ROC-AUC: {comparison_df.loc[best_name, 'roc_auc']:.4f}\n",
    "  - Precision: {comparison_df.loc[best_name, 'precision']:.4f}\n",
    "  - Recall: {comparison_df.loc[best_name, 'recall']:.4f}\n",
    "  - Optimal Threshold: {optimal_threshold:.2f}\n",
    "\n",
    "TOP 5 IMPORTANT FEATURES:\n",
    "{importance_df.head().to_string()}\n",
    "\n",
    "ARTIFACTS SAVED:\n",
    "  - Best model: {MODELS_DIR / 'best_model.joblib'}\n",
    "  - Preprocessor: {MODELS_DIR / 'preprocessor.joblib'}\n",
    "  - Feature names: {MODELS_DIR / 'feature_names.joblib'}\n",
    "  - MLflow runs: models/mlflow/\n",
    "  - Figures: reports/figures/\n",
    "''')\n",
    "\n",
    "print('\\nModel training complete! Ready for deployment.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
